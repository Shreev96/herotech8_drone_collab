[RL Parameters]
# max steps
steps = 10
# max episodes
episodes = 1000
# Delay before each training (in episodes)
train_period = 3
# Delay before resetting new start and goal position (in episodes)
start_goal_reset_period = 1

[Grid Parameters]
# Square grid size
grid_size = 5

[Agents Parameters]
# number of agents
n = 1
# RL model used (all the agents have the same)
model = SQN
# TODO: new config file with one section per agent

[SQN Parameters]
# Entropy weighting
alpha = 0.05
# Batch size
batch_size = 512
# Learning rate for the Q-table update equation
learning_rate = 0.01
# Discount factor for the Q-table update equation
discount_factor = 0.99
# step delay before updating target_model
update_period = 10

[DQN Parameters]
# Starting exploration vs Exploitation coefficient for e-greedy algorithm
eps = 0.9
# Batch size
batch_size = 512
# Learning rate for the Q-table update equation
learning_rate = 0.01
# Discount factor for the Q-table update equation
discount_factor = 0.99
# step delay before updating target_model
update_period = 10

[Gridworld Parameters]
# rewards for attempting to enter a cell with:
FREE = -0.04
GOAL = 10.0
OBSTACLE = -0.75
OUT_OF_BOUNDS = -0.8

# TODO: implement wind

