[RL Parameters]
# max steps
steps = 100
# max episodes
episodes = 5000
# Delay before each training (in episodes)
train_period = 5
# Delay before resetting new start and goal position (in episodes)
start_goal_reset_period = 1
# Environmental effects like wind (ON or OFF)
effects = False

[Grid Parameters]
# Square grid size
grid_size = 10

[Agents Parameters]
# number of agents
n = 1
# RL model used (all the agents have the same)
model = SQN
# TODO: new config file with one section per agent

[SQN Parameters]
# Entropy weighting
alpha = 2.0
# Batch size
batch_size = 512
# Learning rate for the Q-table update equation
learning_rate = 0.001
# Discount factor for the Q-table update equation
discount_factor = 0.99
# step delay before updating target_model
update_period = 3

[DQN Parameters]
# Starting exploration vs Exploitation coefficient for e-greedy algorithm
eps = 0.9
# Batch size
batch_size = 512
# Learning rate for the Q-table update equation
learning_rate = 0.02
# Discount factor for the Q-table update equation
discount_factor = 0.99
# step delay before updating target_model
update_period = 5

[Gridworld Parameters]
# rewards for attempting to enter a cell with:
FREE = -0.01
GOAL = 10.0
OBSTACLE = -0.75
OUT_OF_BOUNDS = -0.01

[Environmental Effects]
col_wind = [0, 1, 1, 0, 0]
range_random_wind = 1
probabilities = [0.2, 0.3, 0.5]